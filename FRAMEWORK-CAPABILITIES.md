# ðŸ—ï¸ Framework Capabilities Report

**Analysis of:** 609 tests across 26 agents (Benchmark 20260206-015725)

---

## ðŸ“Š Quick Summary

| Framework | Speed | Streaming | Tools | Tokens | State | Success | Best For |
|-----------|-------|-----------|-------|--------|-------|---------|----------|
| **PydanticAI** | âš¡âš¡âš¡ | âœ… 81K c/s | âœ… | âŒ | âŒ | 100% | Fast, reliable, multi-model |
| **Agno** | âš¡âš¡âš¡ | âœ… 69K c/s | âœ… | âŒ | âŒ | 98% | Ultra-fast with tools |
| **LangGraph** | âš¡âš¡âš¡ | âœ… 21K c/s | âœ… | âœ… | âœ… | 89% | Complex workflows + observability |
| **Vercel AI SDK** | âš¡âš¡ | âœ… 65K c/s | âœ… | âŒ | âŒ | 100% | TypeScript, multi-model |
| **LlamaIndex** | âš¡âš¡âš¡ | âŒ | âŒ | âŒ | âœ… | 96% | State management, RAG |
| **Anthropic Raw** | âš¡ | âœ… 16K c/s | âœ… | âœ… | âŒ | 100% | Token tracking, direct API |
| **OpenAI Raw** | ðŸ¢ | âœ… 3.5K c/s | âœ… | âŒ | âŒ | 100% | Direct API (slow) |
| **Gemini Raw** | âš¡ | âœ… 72K c/s | âœ… | âŒ | âŒ | 100% | Fast direct API |
| **Cerebras** | âš¡âš¡âš¡âš¡ | âœ… 44K c/s | âŒ | âŒ | âŒ | 100% | Ultra-fast inference |
| **CrewAI** | ðŸ¢ | âŒ | âŒ | âŒ | âœ… | 100% | Multi-agent orchestration |
| **Google ADK** | âš¡âš¡ | âœ… 195K c/s | âŒ | âŒ | âŒ | 100% | Gemini-native, fastest streaming |
| **AG2** | âš¡ | âŒ | âŒ | âŒ | âŒ | 100% | AutoGen compatibility |

---

## ðŸ¥‡ Performance Champions

### Fastest Streaming
1. **Google ADK**: 195K chars/sec ðŸš€
2. **PydanticAI-Gemini**: 82K chars/sec
3. **Gemini Raw**: 72K chars/sec
4. **Agno-Gemini**: 69K chars/sec
5. **Vercel-Gemini**: 65K chars/sec

**Insight:** Gemini-based agents dominate streaming performance!

### Fastest Response Time
1. **Agno-Cerebras**: 489ms average âš¡
2. **LlamaIndex-Anthropic**: 1,744ms
3. **PydanticAI-Anthropic**: 1,794ms
4. **Agno-Anthropic**: 2,325ms
5. **LangGraph-Cerebras**: 2,422ms

**Insight:** Cerebras models are 3-5x faster than GPT/Claude!

### Most Reliable (100% Success)
- AG2
- Agno-Anthropic, Agno-Cerebras, Agno-OpenAI
- Anthropic Raw, OpenAI Raw, Gemini Raw
- All Cerebras variants
- CrewAI
- Google ADK
- All Vercel AI SDK variants
- All PydanticAI variants

---

## ðŸ“‹ Detailed Framework Analysis

### ðŸŒŸ PydanticAI (Python, Multi-Model)

**What Works:**
- âœ… **Streaming**: 82K c/s (Gemini), 14K c/s (Claude), 3.4K c/s (GPT)
- âœ… **Tool Calls**: 12-16 tools across all models
- âœ… **Multi-Model**: Claude, GPT, Gemini all supported
- âœ… **Reliability**: 100% success rate
- âœ… **Speed**: 1.8-5.1s average response time

**What's Missing:**
- âŒ No token tracking
- âŒ No state snapshots
- âŒ No HITL support detected

**AG-UI Events:**
- RUN_STARTED, RUN_FINISHED
- TEXT_MESSAGE_START, TEXT_MESSAGE_CONTENT, TEXT_MESSAGE_END
- TOOL_CALL_START, TOOL_CALL_ARGS, TOOL_CALL_END, TOOL_CALL_RESULT

**Best For:**
- Production apps needing reliability
- Multi-model support
- Fast response times
- Tool calling

**Limitations:**
- No built-in observability (no state tracking)
- No cost tracking (no tokens)

---

### ðŸ”· Agno (Python, Multi-Model)

**What Works:**
- âœ… **Ultra-Fast**: 489ms with Cerebras!
- âœ… **Streaming**: 69K c/s (Gemini), 14K c/s (Claude), 3.4K c/s (GPT)
- âœ… **Tool Calls**: 12-16 tools
- âœ… **Multi-Model**: Claude, GPT, Gemini, Cerebras
- âœ… **Success**: 98% overall (92.6% with Gemini)

**What's Missing:**
- âŒ No token tracking
- âŒ No state management
- âš ï¸ Gemini model less stable (92.6% vs 100% others)

**AG-UI Events:**
- Standard streaming events (RUN, TEXT_MESSAGE, TOOL_CALL)

**Best For:**
- Ultra-fast responses
- Cerebras integration
- Multi-model flexibility
- Tool-heavy applications

**Limitations:**
- Occasional Gemini failures
- No observability features

---

### ðŸ”µ LangGraph (Python, Multi-Model)

**What Works:**
- âœ… **State Management**: STATE_SNAPSHOT events
- âœ… **Token Tracking**: Full usage metadata
- âœ… **Multi-Model**: Claude, GPT, Gemini, Cerebras
- âœ… **Streaming**: 21K c/s (Gemini), 16K c/s (Cerebras), 6K c/s (Claude)
- âœ… **Observability**: RAW events with internal details
- âœ… **Step Tracking**: STEP_STARTED, STEP_FINISHED

**What's Missing:**
- âš ï¸ Lower reliability: 89% success rate (27/27 tests, 3 failures)
- âš ï¸ Slower than pure frameworks (state tracking overhead)

**AG-UI Events:**
- Complete set: RUN, TEXT_MESSAGE, TOOL_CALL
- Advanced: STATE_SNAPSHOT, STEP_STARTED, STEP_FINISHED
- Special: RAW events with framework internals
- MESSAGES_SNAPSHOT, TEXT_MESSAGE_START/CONTENT/END
- TOOL_CALL_START/END/RESULT

**Best For:**
- Complex multi-agent workflows
- Need for state persistence
- Cost tracking (token usage)
- Graph-based agent orchestration
- Debugging (rich RAW events)

**Limitations:**
- Occasional failures (11% error rate)
- Slower than simpler frameworks
- More complex setup

---

### ðŸŒ Vercel AI SDK (TypeScript, Multi-Model)

**What Works:**
- âœ… **Multi-Model**: Claude, GPT, Gemini
- âœ… **Streaming**: 65K c/s (Gemini), 15K c/s (Claude), 3.3K c/s (GPT)
- âœ… **Tool Calls**: 12-15 tools
- âœ… **Reliability**: 100% success
- âœ… **TypeScript Native**: Best TS experience

**What's Missing:**
- âŒ No token tracking
- âŒ No state management

**AG-UI Events:**
- Standard streaming and tool events

**Best For:**
- TypeScript/Node.js apps
- Multi-model support
- Fast streaming
- Tool calling
- Next.js integration

**Limitations:**
- No observability features
- No cost tracking

---

### ðŸ“š LlamaIndex (Python, Multi-Model)

**What Works:**
- âœ… **State Management**: STATE_SNAPSHOT
- âœ… **Multi-Model**: Claude, GPT, Gemini
- âœ… **Fast**: 1.7-5.3s average
- âœ… **Reliability**: 96% success (89% with Claude)
- âœ… **RAG Ready**: Built for retrieval

**What's Missing:**
- âŒ No standard streaming (uses TEXT_MESSAGE_CHUNK instead)
- âŒ No token tracking
- âŒ Uses TOOL_CALL_CHUNK (non-standard)

**AG-UI Events:**
- MESSAGES_SNAPSHOT (instead of standard streaming)
- TEXT_MESSAGE_CHUNK (non-standard)
- TOOL_CALL_CHUNK (non-standard)
- STATE_SNAPSHOT
- RUN_STARTED, RUN_FINISHED

**Best For:**
- RAG applications
- State persistence
- Multi-model flexibility
- Document-heavy workloads

**Limitations:**
- Non-standard AG-UI events
- No streaming text deltas
- Claude less stable (89% vs 100%)

---

### ðŸ”´ Anthropic Raw API

**What Works:**
- âœ… **Token Tracking**: USAGE_METADATA events! â­
- âœ… **Tool Calls**: 15 tools
- âœ… **Streaming**: 16K chars/sec
- âœ… **Reliability**: 100% success
- âœ… **Direct API**: No framework overhead

**What's Missing:**
- âš ï¸ Slow: 8.7s average (2-3x slower than frameworks!)
- âŒ No state management
- âŒ Single model only (Claude)

**AG-UI Events:**
- Complete standard set
- USAGE_METADATA (unique!)

**Best For:**
- Token tracking needed
- Direct Anthropic control
- Research/debugging
- Cost optimization

**Limitations:**
- Much slower than frameworks (8.7s vs 2-5s)
- Claude-only
- No multi-model

**Insight:** Frameworks are 2-4x FASTER than raw API!
- Framework: 2.3s (Agno-Anthropic)
- Raw API: 8.7s (Anthropic Raw)
- **Speedup: 3.8x** from using framework!

---

### ðŸŸ¢ OpenAI Raw API

**What Works:**
- âœ… **Tool Calls**: 12 tools
- âœ… **Streaming**: 3.5K chars/sec
- âœ… **Reliability**: 100% success

**What's Missing:**
- ðŸ¢ **Very Slow**: 22.7s average! (10x slower than frameworks!)
- âŒ No token tracking (stream_options issue)
- âŒ Single model only (GPT)

**AG-UI Events:**
- Standard streaming and tool events

**Best For:**
- Direct OpenAI API control
- Debugging

**Limitations:**
- Extremely slow (22s vs 5s for frameworks!)
- No multi-model
- No token tracking

**Insight:** Frameworks are 4-5x FASTER!
- Framework: 5.1s (PydanticAI-OpenAI)
- Raw API: 22.7s (OpenAI Raw)
- **Speedup: 4.4x** from using framework!

---

### ðŸŸ£ Gemini Raw API

**What Works:**
- âœ… **Fast Streaming**: 72K chars/sec
- âœ… **Tool Calls**: 13 tools
- âœ… **Reliability**: 100% success

**What's Missing:**
- âš ï¸ Slow: 8.5s average (vs 2.8s for frameworks)
- âŒ No token tracking
- âŒ Single model only

**AG-UI Events:**
- Standard streaming and tool events

**Best For:**
- Direct Gemini API control
- Fast streaming needed

**Limitations:**
- 3x slower than frameworks
- No multi-model

---

### âš¡ Cerebras (Ultra-Fast Inference)

**What Works:**
- âœ… **Ultra-Fast**: 3.4-4.8s average
- âœ… **Fastest Streaming**: 44K chars/sec
- âœ… **Reliability**: 100% success
- âœ… **Multiple Models**: llama-3.3-70b, llama-3.1-70b, llama-3.1-8b

**What's Missing:**
- âŒ No tool calling
- âŒ No token tracking
- âŒ No state management
- âŒ Text generation only

**AG-UI Events:**
- Basic: RUN_STARTED, RUN_FINISHED
- TEXT_MESSAGE_START, TEXT_MESSAGE_CONTENT, TEXT_MESSAGE_END

**Best For:**
- Speed-critical applications
- Simple text generation
- High throughput
- Low latency requirements

**Limitations:**
- No tools
- No observability
- Basic functionality only

---

### ðŸŸ  CrewAI (Multi-Agent Orchestration)

**What Works:**
- âœ… **State Management**: STATE_SNAPSHOT
- âœ… **Multi-Agent**: MESSAGES_SNAPSHOT pattern
- âœ… **Reliability**: 100% success
- âœ… **Step Tracking**: STEP_STARTED, STEP_FINISHED

**What's Missing:**
- ðŸ¢ **Slow**: 14.1s average
- âŒ No streaming (MESSAGES_SNAPSHOT only)
- âŒ No token tracking
- âŒ No tool call events

**AG-UI Events:**
- MESSAGES_SNAPSHOT (batch updates)
- STATE_SNAPSHOT
- STEP_STARTED, STEP_FINISHED
- RUN_STARTED, RUN_FINISHED

**Best For:**
- Multi-agent workflows
- Crew/team orchestration
- Complex task delegation
- Batch processing

**Limitations:**
- Slowest framework (14s)
- No streaming text
- No standard tool events

---

### ðŸ”µ Google ADK (Gemini-Native)

**What Works:**
- âœ… **Fastest Streaming**: 195K chars/sec! ðŸš€ðŸš€ðŸš€
- âœ… **Fast**: 4.6s average
- âœ… **Reliability**: 100% success
- âœ… **Gemini-Optimized**: Native integration

**What's Missing:**
- âŒ No tool calling
- âŒ No token tracking
- âŒ No state management
- âŒ Gemini-only

**AG-UI Events:**
- Standard streaming events
- RUN_ERROR (good error handling)

**Best For:**
- Gemini-native applications
- Ultra-fast streaming
- Simple text generation
- High-throughput use cases

**Limitations:**
- Single model (Gemini only)
- No tools
- No observability

---

### ðŸŸ¤ AG2/AutoGen

**What Works:**
- âœ… **Reliability**: 100% success
- âœ… **AutoGen Compatible**: Smooth migration path

**What's Missing:**
- âš ï¸ Slow: 8.2s average
- âŒ No streaming (basic events only)
- âŒ No token tracking
- âŒ No state management
- âŒ No tool call events

**AG-UI Events:**
- Basic: RUN_STARTED, RUN_FINISHED, RUN_ERROR
- TEXT_MESSAGE_START, TEXT_MESSAGE_CONTENT, TEXT_MESSAGE_END

**Best For:**
- AutoGen migration
- Simple use cases

**Limitations:**
- Slowest without benefits
- Missing most AG-UI features
- Basic functionality only

---

## ðŸŽ¯ Feature Comparison Matrix

| Feature | PydanticAI | Agno | LangGraph | Vercel | LlamaIndex | Raw APIs | Cerebras | CrewAI | ADK | AG2 |
|---------|------------|------|-----------|--------|------------|----------|----------|--------|-----|-----|
| **Streaming** | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âœ… | âŒ | âœ… | âŒ |
| **Tools** | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| **Tokens** | âŒ | âŒ | âœ… | âŒ | âŒ | âš ï¸ | âŒ | âŒ | âŒ | âŒ |
| **State** | âŒ | âŒ | âœ… | âŒ | âœ… | âŒ | âŒ | âœ… | âŒ | âŒ |
| **Multi-Model** | âœ… | âœ… | âœ… | âœ… | âœ… | âŒ | âš ï¸ | âŒ | âŒ | âŒ |
| **Speed** | âš¡âš¡âš¡ | âš¡âš¡âš¡ | âš¡âš¡ | âš¡âš¡ | âš¡âš¡âš¡ | ðŸ¢ | âš¡âš¡âš¡âš¡ | ðŸ¢ | âš¡âš¡ | ðŸ¢ |
| **Reliability** | 100% | 98% | 89% | 100% | 96% | 100% | 100% | 100% | 100% | 100% |
| **TypeScript** | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ | âŒ |

---

## ðŸ† Recommendations

### For Production Apps
**Winner: PydanticAI**
- 100% reliability
- Fast (1.8-5.1s)
- Multi-model
- Tool support
- Good streaming

### For Cost Optimization
**Winner: LangGraph**
- Only framework with full token tracking
- State management for debugging
- Good observability

### For Speed
**Winner: Agno + Cerebras**
- 489ms average!
- 10-30x faster than alternatives
- But no tools

### For TypeScript
**Winner: Vercel AI SDK**
- Native TypeScript
- Multi-model
- Good streaming
- 100% reliable

### For Complex Workflows
**Winner: LangGraph**
- State management
- Step tracking
- Multi-agent support
- Best observability

### For RAG
**Winner: LlamaIndex**
- Built for retrieval
- State management
- Fast (1.7-5.3s)

---

## ðŸ’¡ Key Insights

1. **Frameworks >> Raw APIs**
   - 2-10x faster
   - Better reliability
   - More features
   - **Never use raw APIs in production!**

2. **Gemini = Fastest Streaming**
   - Google ADK: 195K c/s
   - PydanticAI-Gemini: 82K c/s
   - Gemini Raw: 72K c/s
   - 20-60x faster than GPT streaming!

3. **Cerebras = Fastest Inference**
   - 489ms average (Agno)
   - 3-10x faster than GPT/Claude
   - But no tools yet

4. **Token Tracking = Rare**
   - Only LangGraph has full support
   - Anthropic Raw partial support
   - 19% coverage overall
   - **Major gap in ecosystem!**

5. **LangGraph = Most Failures**
   - 11% failure rate vs 0-4% others
   - But most features
   - Trade-off: capability vs reliability

6. **CrewAI = Different Pattern**
   - No streaming
   - MESSAGES_SNAPSHOT instead
   - Multi-agent focus
   - Slowest (14s)

---

## ðŸš¨ What's Missing Everywhere

### HITL (Human-in-the-Loop)
- **0 frameworks** emit HITL_PROMPT/HITL_RESPONSE events
- Test scenarios don't trigger human approval
- Framework support unclear

### Artifacts
- **0 frameworks** emit ARTIFACT events
- No code generation artifacts detected
- Need better test prompts

### Error Recovery
- No frameworks emit retry/recovery events
- Error handling not observable
- Black box on failures

---

## ðŸ“ˆ Success Rates

**Perfect (100%):**
- PydanticAI (all models)
- Vercel AI SDK (all models)
- All Raw APIs
- All Cerebras
- CrewAI
- Google ADK
- AG2
- Agno-Anthropic, Agno-Cerebras, Agno-OpenAI

**Good (95-99%):**
- LlamaIndex: 96% overall
  - 89% with Anthropic
  - 100% with OpenAI/Gemini

**Needs Work (< 95%):**
- Agno-Gemini: 92.6%
- LangGraph: 89% (all models)
  - Consistent failures across Claude, GPT, Gemini

**Insight:** LangGraph's complexity leads to more failures, but also more capabilities.

---

**Last Updated:** 2026-02-06
**Benchmark:** 609 tests across 26 agents
**Coverage:** 12 frameworks, 4 model providers
